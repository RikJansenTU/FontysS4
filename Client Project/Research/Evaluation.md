### General Research Evaluation
While I definitely expected the AI generated imagery to have a positive effect on ideation, I didn't expect it to be quite so pronounced. The drawings people made proved quite a good way to quantify something as vague as 'creativity', but being able to see what elements they added to their second concept, and how often they directly corresponded to elements in the AI images, really helped pin down which images served as the best inspiration.

The most difficult part of setting up this user research was knowing what subject to pick for the concepts. I first wanted to be more specific, and ask people to create a christmas-themed advertising poster for an electric car, which I hoped would deliver more quantifiable results, but in the end we decided to go with something simpler, so as not to overwhelm people. I do think this worked out, since for some subjects merely having to draw something in a limited time was already a barrier of entry, and while the concept was simple there was still plenty that could be added.

### Effect on Future Prototypes
Determining what images had the most effect was very important; inspiring people can be a pretty daunting task, and the more handholds we get in knowing how to do it the better. When Greenhouse employees use our room to, for example, ask for inspiration on chairs, it won't be very useful to just show them slightly weird pictures of chairs. Inspiration often springs from combining multiple ideas, and our room has the potential to provide those ideas. So, based on the research results, there are two major additions I want to make to the prototype:

1. More related imagery. This is something that might prove difficult, considering you can't just ask an AI to create pictures closely related to a subject, but seeing the research results it's something well worth pursuing. I have already started looking into word association libraries. These are also AI driven, and I hope to be able to feed them the prompt and receive a bunch of related words; say the prompt is 'chair', it will return things like 'table', 'living room', etc. Utilising those words could create some very interesting results, especially if we manage to craft our prompts to the AI in an interesting way. I imagine creating and fine-tuning this will be a great challenge for next sprint.
2. Stylistic changes. This is something that is easier to do, and I have already started implementing it in our prototype. There are some [great guides on how to get interesting stylistic results with DALL-E](https://docs.google.com/document/d/11WlzjBT0xRpQhP9tFMtxzd0q6ANIdHPUBkMV-YB043U/edit#), and using those I am working on creating something that will, every once in a while, change around the artstyle of an image. Not only will this result in greater variation in results, but also (hopefully) serve as stylistic inspiration, something the research proved is possible.
